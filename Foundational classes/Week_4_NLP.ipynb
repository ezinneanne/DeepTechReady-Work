{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5FHkJ9VsEg4YLo/Vnutlx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezinneanne/DeepTechReady-Work/blob/new_branch/Week_4_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1"
      ],
      "metadata": {
        "id": "HTpJGRH1HkJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Define Natural Language Processing (NLP) in your own words.\n",
        "\n",
        "     Answer: Natural Language Processing(NLP) is a subset of machine learning that involves machines being able to know, interpret human communication and provide a suitable response."
      ],
      "metadata": {
        "id": "1Gp1mi_w3myf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   List at least three real-world applications of NLP and explain their significance.\n",
        "\n",
        "     Answer: Google/Siri Assistant, Google Translate, Email spam filters. Their significance includes:\n",
        "     - Providing reliable information to users, helping users perform tasks using user voice commands only.\n",
        "     - Translating languages for users enabling smooth flow of communication.\n",
        "     - Being able to spot out spam messages by identifying certain words in a mail head or body of the mail."
      ],
      "metadata": {
        "id": "itU6Nxiz8P2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.   Identify and explain two challenges that make NLP complex.\n",
        "\n",
        "     Answer: Ambiguity and noisy data.\n",
        "\n",
        "     Ambiguous words i.e words without a clear meaning can be complex for machines to understand. An example is the sentence in the course material \"I saw the man with the telescope\" this is ambiguous as the man is not known to the machine. Another example is \"We  saw her duck\", this is unclear too as the machine will struggle to understand if it was her duck or her dodging something.\n",
        "\n",
        "     Noisy words are usually found on social media where people communicate using slangs and acronyms. Examples are \"See u 2morrow\" and \"Tysm!\". Now, these words are difficult for a machine to interpret them correctly."
      ],
      "metadata": {
        "id": "I3FzM79M8aYD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zfj0zyEi2dKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69ef8d62-8534-45fb-9539-a33fe85d03c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emails found:  ['support@company.com', 'sales@business.org', 'info@service.net']\n"
          ]
        }
      ],
      "source": [
        "# importing the regex library\n",
        "import re\n",
        "\n",
        "# Using re.findall() to extract email address\n",
        "text = \"Contact us at support@company.com or sales@business.org. For more, email info@service.net.\"\n",
        "\n",
        "emails = re.findall(r'\\b[\\w.-]+@[\\w.-]+\\.\\w+\\b', text)\n",
        "\n",
        "print(\"Emails found: \", emails)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using re.findall() to extract words that end with \"ing\" in a sentence\n",
        "sentence = \"NLP is amazing for cleaning and processing text while learning new techniques.\"\n",
        "\n",
        "ing_words = re.findall(r'\\b\\w+ing\\b', sentence)\n",
        "\n",
        "print(\"Words ending with 'ing': \", ing_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfRynMTcD1nH",
        "outputId": "0f64a6f4-c8ee-4a9f-fd7a-10e2d39467c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words ending with 'ing':  ['amazing', 'cleaning', 'processing', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A python program to clean a text by removing all punctuation, converting it to lowercase, and splitting it to words.\n",
        "def clean_text(text):\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Split into words\n",
        "    words = text.split()\n",
        "    return words\n",
        "\n",
        "text = \"NLP makes AI smarter! But, sometimes, it‚Äôs challenging... Don‚Äôt you agree?\"\n",
        "cleaned_words = clean_text(text)\n",
        "print(\"Cleaned words: \",cleaned_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by1lCWJOFDMu",
        "outputId": "331a30f2-0c41-4b74-c0b9-1a440083e356"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned words:  ['nlp', 'makes', 'ai', 'smarter', 'but', 'sometimes', 'its', 'challenging', 'dont', 'you', 'agree']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2"
      ],
      "metadata": {
        "id": "rAYjZTcdH_Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying text cleaning techniques to preprocess text\n",
        "text = \"OMG!! NLP is soooo coool ü§©...!!! It costs $1000. Learn it now at https://3mtt.com üòé.\"\n",
        "# first applying lowercasing\n",
        "text = text.lower()\n",
        "print(text)\n",
        "# second removing punctuation and links\n",
        "text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "text = re.sub(r'[^a-z\\s]', '', text)\n",
        "text = text.strip()\n",
        "print(text)\n",
        "words = text.split()\n",
        "print(words)\n",
        "# third removing stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "def normalize_word(word):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', word)\n",
        "\n",
        "final_words = [normalize_word(word) for word in filtered_words]\n",
        "\n",
        "cleaned_text = \" \".join(final_words)\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QGEpkHaI5kF",
        "outputId": "fe0d29c7-4a03-41db-89cf-2f9e5fc18ced"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "omg!! nlp is soooo coool ü§©...!!! it costs $1000. learn it now at https://3mtt.com üòé.\n",
            "omg nlp is soooo coool  it costs  learn it now at\n",
            "['omg', 'nlp', 'is', 'soooo', 'coool', 'it', 'costs', 'learn', 'it', 'now', 'at']\n",
            "omg nlp so col costs learn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word tokenization\n",
        "nltk.download('punkt_tab')\n",
        "text = \"Tokenization is the first step in NLP. It splits text into smaller pieces for analysis.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OHSZ_9qiB9D",
        "outputId": "dbc03637-4a8a-4190-f706-61b34d0ce67e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP', '.', 'It', 'splits', 'text', 'into', 'smaller', 'pieces', 'for', 'analysis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence tokenization\n",
        "text = \"Tokenization is the first step in NLP. It splits text into smaller pieces for analysis.\"\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7nSh9wSjQgu",
        "outputId": "a25272f8-74dd-49cd-bdd8-0ab0aa5360fb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization is the first step in NLP.', 'It splits text into smaller pieces for analysis.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using PorterStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# words\n",
        "words = [\"running\", \"flies\", \"studies\", \"easily\", \"studying\", \"better\"]\n",
        "\n",
        "# Apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(\"Stemmed Words:\", stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HiuAGAOj_Bp",
        "outputId": "481fe980-c890-414d-9c31-812accd22d2e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['run', 'fli', 'studi', 'easili', 'studi', 'better']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using spacy for lemmatization\n",
        "# Step 1: Upgrade pip, setuptools, and wheel\n",
        "!pip install -U pip setuptools wheel\n",
        "\n",
        "# Step 2: Install spaCy\n",
        "!pip install -U spacy\n",
        "\n",
        "# Step 3: Download the English model\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2aRJsvGUk2jz",
        "outputId": "95bad969-2567-4a12-971a-094d5de23151"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.2.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-78.1.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-78.1.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-25.0.1 setuptools-78.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "513001d23ed842f3bf8e48822c9629bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.8.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (78.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.0)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading spacy-3.8.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.8.4\n",
            "    Uninstalling spacy-3.8.4:\n",
            "      Successfully uninstalled spacy-3.8.4\n",
            "Successfully installed spacy-3.8.5\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# words\n",
        "words = [\"running\", \"flies\", \"studies\", \"easily\", \"studying\", \"better\"]\n",
        "\n",
        "# Apply lemmatization\n",
        "lemmatized_words = [nlp(word)[0].lemma_ for word in words]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zid-Fvzem-wr",
        "outputId": "a3ef02a9-1f4c-4211-b49d-37a45d7f74dc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: ['run', 'fly', 'study', 'easily', 'study', 'well']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 3"
      ],
      "metadata": {
        "id": "iz8iewTaoWkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "#5 word vocabulary\n",
        "vocabulary = [\"anger\", \"sorrow\", \"sadness\", \"pity\", \"sympathy\"]\n",
        "# Converting words to numerical indices\n",
        "data = np.array(vocabulary).reshape(-1, 1)\n",
        "\n",
        "# One-Hot Encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "one_hot = encoder.fit_transform(data)\n",
        "\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"One-Hot Encoded Vectors:\\n\", one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifdr0NYYoePG",
        "outputId": "c17cb297-e32a-4b4d-83d1-926c27346dff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['anger', 'sorrow', 'sadness', 'pity', 'sympathy']\n",
            "One-Hot Encoded Vectors:\n",
            " [[1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using bag of words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#example dataset\n",
        "dataset = [\"The quick brown fox jumps over the lazy dog.\", \"The dog sleeps in the kernel\"]\n",
        "\n",
        "#create a BoW(bag of words) model\n",
        "vectorizer = CountVectorizer()\n",
        "bag_of_words = vectorizer.fit_transform(dataset)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"Bag of Words:\\n\", bag_of_words.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSLeLVMVsGu1",
        "outputId": "47bc1229-c593-4e18-a186-45b8b22933ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['brown' 'dog' 'fox' 'in' 'jumps' 'kernel' 'lazy' 'over' 'quick' 'sleeps'\n",
            " 'the']\n",
            "Bag of Words:\n",
            " [[1 1 1 0 1 0 1 1 1 0 2]\n",
            " [0 1 0 1 0 1 0 0 0 1 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#example dataset\n",
        "dataset = [\"The quick brown fox jumps over the lazy dog.\", \"The dog sleeps in the kernel\"]\n",
        "\n",
        "#create a TF-IDF model\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf = vectorizer.fit_transform(dataset)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF:\\n\", tfidf.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg7sUbcNtrbv",
        "outputId": "5d07559e-4a97-494a-ccfc-a4e99b9f8cfa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['brown' 'dog' 'fox' 'in' 'jumps' 'kernel' 'lazy' 'over' 'quick' 'sleeps'\n",
            " 'the']\n",
            "TF-IDF:\n",
            " [[0.342369   0.24359836 0.342369   0.         0.342369   0.\n",
            "  0.342369   0.342369   0.342369   0.         0.48719673]\n",
            " [0.         0.30253071 0.         0.42519636 0.         0.42519636\n",
            "  0.         0.         0.         0.42519636 0.60506143]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITVNCSdswEZ5",
        "outputId": "2b2bb1c0-68c2-4e2d-bbc6-17ea2a808714"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m133.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using Word2Vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# creating dataset of animals\n",
        "animals_dataset = [[\"the\", \"dog\", \"barks\"], [\"the\", \"cat\", \"meows\"], [\"the\", \"cow\", \"moos\"], [\"the\", \"bird\", \"chirps\"], [\"the\", \"duck\", \"quacks\"]]\n",
        "\n",
        "# Train Word2Vec Model\n",
        "word2vec_model = Word2Vec(animals_dataset, vector_size=10, window=2, min_count=1, workers=4)\n",
        "\n",
        "# Get Embedding for a Word\n",
        "print(\"Word Embedding for 'dog':\\n\", word2vec_model.wv['dog'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNPsBDjPubYM",
        "outputId": "3961614d-5754-46fc-f08e-a91ef151c0b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Embedding for 'dog':\n",
            " [-0.08619688  0.03665738  0.05189884  0.05741938  0.07466918 -0.06167675\n",
            "  0.01105614  0.06047282 -0.0284005  -0.06173522]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading pretrained Glove model\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load Pretrained GloVe Model\n",
        "glove_model = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "# Get Embedding for a Word\n",
        "print(\"Word Embedding for 'king':\\n\", glove_model['king'])\n",
        "\n",
        "# Find the 5 most similar words to \"king\"\n",
        "similar_words = glove_model.most_similar('king', topn=5)\n",
        "print(\"\\nTop 5 words similar to 'king':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"{word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f63dcT1EzEdT",
        "outputId": "481e9ace-d145-4f53-8ca5-8bdbefcf0655"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "Word Embedding for 'king':\n",
            " [ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n",
            "  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173\n",
            "  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961\n",
            " -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783\n",
            " -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159\n",
            "  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685\n",
            " -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426\n",
            " -0.51042 ]\n",
            "\n",
            "Top 5 words similar to 'king':\n",
            "prince: 0.8236\n",
            "queen: 0.7839\n",
            "ii: 0.7746\n",
            "emperor: 0.7736\n",
            "son: 0.7667\n"
          ]
        }
      ]
    }
  ]
}